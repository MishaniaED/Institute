\documentclass[a4paper,14pt]{extarticle}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[dvips]{graphicx}
\usepackage{color}
\usepackage[dvips]{hyperref}
\usepackage{pst-all}

\usepackage{setspace}
\usepackage{indentfirst}
\usepackage{textcomp}
\usepackage{ifthen}
\usepackage{calc}

\usepackage{amsmath}

\usepackage[cache=false]{minted}
\hypersetup{%
	unicode,%
	linkcolor=blue,
	colorlinks=true,%
%	pdfpagemode=FullScreen,%
%	pdfpagetransition=Dissolve,%
	pdftitle={Курсовая работа по дисциплине "Теория Принятия Решений"},%
}


\thispagestyle{empty}

\setlength{\voffset}{-.8in}
\setlength{\hoffset}{-.75in}
\addtolength{\textheight}{1.6in}
\addtolength{\textwidth}{1.5in}


\onehalfspacing

\setcounter{tocdepth}{2}
\begin{document}

\begin{center}
{	
	\vfill \textsc{\Large курсовая работа} \\
	{\large по дисциплине <<Теория Принятия Решений>>}
 
        \bigskip
        
        \textbf{Вариант 9}
 }
\end{center}
\vspace*{1.5cm}

\hfill 
\begin{minipage}{.6\linewidth}\
\begin{tabular}{c}

\textbf{Выполнили:} студенты группы М8О-411Б-19 \\\hline \\[.3cm]
{\large Сергеев Владислав Алексеевич} \\ \hline \scriptsize{(Фамилия, имя, отчество)}
\\[.3cm]
{\large Евкарпиев Михаил Дмитриевич} \\ \hline \scriptsize{(Фамилия, имя, отчество)}
\\[.3cm]
{\large Янковский Вадим Владиславович} \\ \hline \scriptsize{(Фамилия, имя, отчество)}
\\[.3cm]

\end{tabular}
\vspace*{1cm}
\end{minipage}		

\vspace*{1cm}

\newpage

\section{Задача №1}
\subsection{Постановка задачи}

Рассмотрим задачу принятия с помощью экспертов, длящуюся $T$ раундов, в которой потери принадлежат $[0, a], a > 0$. Придумайте алгоритм, для которого общие ожидаемые потери 
оцениваются снизу величиной $\max\limits_{k=1,\dots,N} L_{k,T}-c\cdot\sqrt{T\log N}$.

Где $c$ – наилучшая константа, которую вы можете получить, независящая от числа раундов $T$ и
числа экспертов $N$.

\subsection{Решение}

Для решения данной задачи можно использовать алгоритм Hedge, который представляет собой
алгоритм для принятия решений в условиях неопределенности.

Алгоритм Hedge основывается на идее распределения между экспертами весов, пропорциональных их прошлым успехам и неудачам, и использовании этих весов для принятия решений. После каждого раунда наблюдаемые потери используются для обновления весов. Действия, которые демонстрируют хорошие результаты, получают более высокие веса, а те, что плохо справляются - более низкие. Для определения распределения по действиям и выбора действий в соответствии с этим распределением, алгоритм использует полученные веса. Скорость обновления весов управляется параметром скорости обучения $\eta = c \cdot \sqrt{\frac{\log N}{t}}$.

Суть алгоритма в следующем: в первом туре мнения всех экспертов имеют одинаковый вес. Лицо, принимающее решение, примет первое решение, основываясь на прогнозе большинства экспертов. Затем в каждом последующем раунде лицо, принимающее решение, будет неоднократно обновлять вес мнения каждого эксперта в зависимости от правильности его предыдущих прогнозов.

Алгоритм Hedge для $T$ раундов и $N$ экспертов будет выглядеть следующим образом:
\begin{enumerate}
    \item Инициализировать веса всех экспертов равными значениями: $w_{k,1} = 1$ для всех $k$ от $1$ до $N$.

\item На каждом раунде $t = 1,2,\dots,T$:
\begin{itemize}
    \item Вычислить нормализованные веса экспертов:

    Для каждого эксперта $k$ вычисляем нормализованный вес:
    $$w_{k,t} = \frac{w_{k,t}}{\sum_{i=1}^{N}w_{k,t}}$$
    \item Каждый эксперт k объевляет свои потери $L_{k,t} \in [0, a]$, которые он понес в результате принятого решения на предыдущем раунде.

    \item Вычисляется взвешенное решение, используя веса экспертов. На каждом раунде, взвешенное решение вычисляется как сумма нормализованных весов экспертов, на их сумму:
    \begin{equation}
    p_{t} = \frac{(w_{1,t}, w_{2,t},\dots,w_{N,t})}{\sum_{k=1}^N w_{k,t}}.
    \end{equation}
    \item Принять решение. После вычисления взвешенного решения, на текущем раунде выбирается эксперт с наибольшим весом:
    \begin{equation}a_{t} = \underset{k=1,\dots,N}{\arg\max}p_{k,t}.\end{equation}
    \item Обновить веса экспертов:
    \begin{equation}
     w_{k,t} = w_{k,t-1}  \cdot e^{(-\eta \cdot L_{k,t})},
     \end{equation} где $\eta = c \cdot \sqrt{\frac{\log N}{t}}$, а $L_{k,t}$ – потери эксперта $k$ на раунде $t$.
\end{itemize}
    \item После $T$ раундов, алгоритм оценивает общие ожидаемые потери. Это делается, суммируя потери выбранного эксперта на каждом раунде и добавляя к этому сумму взвешенных потерь всех экспертов на последнем раунде. Эта взвешенная сумма выражается следующей формулой:\begin{equation} L_T = \frac{\sum_{t=1}^T L_{a_t,t} + \sum_{k=1}^N w_{k,T} \cdot \log(\frac{1}{w_{k,T}})}{\eta},\end{equation}
    где первый член суммирует потери выбранного эксперта на каждом раунде, а второй член суммирует взвешенные потери всех экспертов на последнем раунде

\end{enumerate}

\subsubsection{Доказательство 1}

Доказательство ограниченности общих ожидаемых потерь снизу величиной
\begin{equation}\max\limits_{k=1,...,N} L_{k,T}-c\cdot\sqrt{T\log N}\end{equation}


Используя неравенство Йенсена, можно доказать, что ожидаемые потери $L_T$ ограничены снизу
величиной $\max\limits_{k=1,...,N} L_{k,T}-c\cdot\sqrt{T\log N}$:

$$L_T = \frac{\sum_{t=1}^T L_{a_t,t} + \sum_{k=1}^N w_{k,T} \cdot \log(\frac{1}{w_{k,T}})}{\eta}
\newline
\leq \frac{\sum_{t=1}^T \max\limits_{k=1,...,N} L_{k,t} + \sum_{k=1}^N w_{k,T} \cdot \log(\frac{1}{w_{k,T}})}{\eta}\leq$$
\newline
$$\leq \frac{\max\limits_{k=1,...,N} \sum_{t=1}^T L_{k,t} + \sum_{k=1}^N w_{k,T} \cdot \log(\frac{1}{w_{k,T}})}{\eta}
\newline
\leq \frac{\max\limits_{k=1,...,N} \sum_{t=1}^T L_{k,t} + \log N}{\eta}\leq$$
\newline
$$\leq \max\limits_{k=1,...,N} L_{k,T} + c \cdot \sqrt{(T \log N)},$$
\newline

Таким образом, оценка общих ожидаемых потерь $L_T$ ограничена снизу величиной $\max\limits_{k=1,...,N} L_{k,T}-c\cdot\sqrt{T\log N}$, что и требовалось доказать.

\subsubsection{Доказательство 2}

Доказательство:

Пусть ожидаемые потери $L_{k,t}$ для эксперта k в момент времени t, $p_{k,t}$ - вероятность, приписываемая действию k-го эксперта в момент времени t алгоритмом Hedge.
Тогда, ожидаемые потери алгоритма Hedge в момент времени t будут равны:

\begin{equation}L_t = \sum_{k=1}^N(p_{k,t} \cdot L_{k,t})\end{equation}

Общие ожидаемые потери алгоритма Hedge за T раундов будут равны:

\begin{equation}L_T = \sum_{t=1}^T{L_t}\end{equation}

Теперь пусть $L_{k,T}$ - ожидаемая потеря эксперта k за T раундов. Ожидаемая потеря наилучшего эксперта определяется следующим образом:

\begin{equation}L^*_T = \min\limits_{k=1,\dots,N}(L_{k,T}).\end{equation}

Regret алгоритма Hedge определяется как (Разница между ожидаемыми потерями алгоритма и наилучшего эксперта определяет показатель regret):

\begin{equation}R_T = L_T - L^*_T\end{equation}

Ключевая идея доказательства нижнего предела для $L_T$ заключается в использовании того факта, что энтропия распределения действий ограничена сверху $\log N$. Это означает, что энтропийный член в ограничении на ожидаемые потери алгоритма Hedge не более $\log N$. Используя этот факт, мы можем показать, что Regret алгоритма Hedge ограничен снизу:

\begin{equation}R_T \geq -c \cdot \sqrt{T \log N}\end{equation}

где c - константа, зависящая от диапазона потерь и энтропии распределения действий.

Для доказательства этого ограничения мы используем лемму.\endgraf
Лемма: Для любого распределения по N действиям энтропия равна не более $\log N$.
Доказательство: Пусть $p_1, \dots, p_N$ - вероятности N действий. Энтропия распределения определяется как:

\begin{equation}H(p_1, \dots, p_N) = -\sum_{k=1}^{N}{p_k \cdot \log p_k}\end{equation}

Используя неравенство $\log x \leq x - 1$, и тот факт, что для любого распределения вероятностей $p_1, \dots, p_N$ справедливо $p_1 + \dots + p_N = 1$ мы можем показать, что:
$$H(p_1, \dots, p_N) = -\sum_{k=1}^{N}{p_k \cdot \log p_k} \leq -\sum_{k=1}^{N}{p_k \cdot (p_k - 1)} = -\sum_{k=1}^{N}{p_k^2 } + \sum_{k=1}^{N}{p_k} =$$
$$
= 1 - \sum_{k=1}^{N}{p_k^2} \leq 1
$$

Таким образом, мы доказали, что энтропия ограничена сверху числом 1.

$$-\sum_{k=1}^{N}{p_k \cdot \log p_k} = \sum_{k=1}^{N}{p_k \cdot \log \frac{1}{p_k}}$$

Далее, мы используем неравенство Йенсена для выпуклой функции $f(x) = \log x$:
$$\sum_{k=1}^{N}{p_k \cdot \log \frac{1}{p_k}} \leq \log\left({\sum_{k=1}^{N}{p_k \cdot \frac{1}{p_k}}}\right) = \log{N}$$

Таким образом энтропия ограничена сверху $\log N$.

Теперь мы можем использовать ограничение Regret для алгоритма Hedge, которое даётся следующим образом:

\begin{equation}R_T \leq \sqrt{2 \cdot T \cdot \log N \cdot H(p_1, \dots, p_N)}\end{equation}

где $H(p_1, \dots, p_N)$ - энтропия распределения действий. Используя приведенную выше лемму, мы можем связать энтропию по $\log N$, что дает:

\begin{equation}R_T \leq \sqrt{2 \cdot T \cdot \log N \cdot \log N} = \sqrt{2 \cdot T} \cdot \log N\end{equation}

Далее нам надо получить нижнюю оценку $R_T$. Заметим, что при выборе $c = \frac{1}{\sqrt{2}}$, получаем:

\begin{equation}R_T \geq -c \cdot \sqrt{T \cdot \log N}.\end{equation}

Комбинируя это с определением Regret, получаем:

\begin{equation}L_T - L^*_T \geq -c \cdot \sqrt{T \cdot \log N}\end{equation}

\begin{equation}L_T \geq L^*_T - c \cdot \sqrt{T \cdot \log N}\end{equation}

Поскольку $L^*_T$ - ожидаемая потеря от наилучшего эксперта. Возможно заменить его на максимально ожидаемую потерю от N действий, что дает:

\begin{equation}L_T \geq \max\limits_{k=1,\dots,N} L_{k,T} - c \cdot\sqrt{T \cdot \log N},\end{equation}

что и требовалось доказать.

Ограничение показывает, что общие ожидаемые потери алгоритма Hedge гарантированно близки к наилучшему фиксированному действию в ретроспективе, вплоть до логарифмического коэффициента числа действий N. Ограничение также зависит от квадратного корня из числа раундов T, что означает, что алгоритм становится более точным с увеличением числа раундов.
\end{document}